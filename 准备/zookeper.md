## zookeeper

Zookeeper 是一个apache发行的，分布式架构的管理、维护框架，其中的数据以节点的形式储存，并且储存量较小，所以适用于一些读多写少的场景，用来保存一些配置相关的信息。

------

#### 下载和安装

------

> 用wget获取压缩包：
>
> ```
> wget https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/apache-zookeeper-3.5.7-bin.tar.gz
> ```
>
> 解压：
>
> ```
> tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz
> ```
>
> 解压后在解压目录下会多出一个名为 `apache-zookeeper-3.5.7-bin` 的文件夹.
>
> 文件夹中包括：
>
> - bin目录 ：可执行文件放置目录
> - conf ：配置文件放置目录
> - docs : 一些教程
> - lib ：依赖的jar包（zookeeper是用java写的）
> - ....

------

#### 入门配置

------

> 在conf目录下会有一个zoo_sample.cfg文件，是配置文件的模版，我们将其更名为zoo.cfg即可生效。
>
> 改文件中的一些配置参数：
>
> - tickTime : 通信心跳时间
> - initLimit : 初始化leader和follower建立通讯的最大时间（几次心跳）
> - syncLimit : 同步通讯最大时间，即非初始化时leader和follower保持通讯的最大时间
> - datadir : Zookeeper中数据保存的位置 (一般来说会做修改)
> - clientPort : 客户端链接端口号

------

#### 一些本地命令

------

```
#启动服务端
./bin/zkServer.sh start
#关闭服务端
./bin/zkServer.sh stop
#启动客户端连接
bin/zkCli.sh
```

------

#### 集群配置：

------

首先, 需要三台服务器.

这里我申请了三台云服务器, 并在每台服务器上按照上述流程安装了zookeeper.

然后，在每个服务器的zkData( zoo.cfg文件中的数据保存目录 )文件夹中新建myid文件,在文件中写入自己的id号(自定义)，这里分别用了2，3，4.

然后在每个zookeeper的zoo.cfg文件中添加如下内容.

```
server.2=[2001:67c:1254:fd::7e25]:2888:3888
server.3=[2001:67c:1254:e:8c:498:0:1]:2888:3888
server.4=[2001:67c:1254:5f:d58a::f2c1]:2888:3888
```

不难看出，配置的格式为：

```
server.{id号}={ip地址}:{端口1}:{端口2}
```

其中，端口1为服务器间通讯用的端口号，端口2为失去链接时，重新选举用的端口号。

------

#### 选举机制

------

##### 第一次选举：

当服务器集群启动时，会进行第一次选举投出第一次的leader，每个服务器启动时会做出如下操作：

1. 先将自己手中的一票投给自己
2. 判断自己手中的选票是否过半，如果过半，自己成为leader，其他服务器默认成为follwer
3. 如果没有过半，则在下一个服务器启动时进行一次比较，将现在已启动服务器中票数最多的那个（也是唯一一个有票的），和现在刚启动的服务器，两者的id进行比较，较小的那个把手中所有的选票交给较大的那个。
4. 结束后，判断手中的选票是否过半，同理，已过半则成为leader，没有的话则继续上述流程。

------

##### 一些概念

------

SID：服务器id，即之前写入的myid

ZXID：每次客户访问的事务id，代表服务器状态变更

Epoch : 每个leader任期的值，每次选举后增加

------

##### 后续选举：

------

当集群中有服务器失去链接时，且失去的是leader时，需要进行重新选举，选举规则如下：

1. 优先比较epoch，大的胜出
2. 然后比较zxid，大的胜出
3. 都一样再比较sid，大的胜出

------

#### 客户端相关操作

------

连接不同(非本地)的集群中其他的服务器：

```
./bin/zkCli.sh server {ip}:2181
```

一些命令：

```
##帮助
help

##查看结点信息 默认跟节点名为 zookeeper
ls /

##更详细信息
ls -s /
## czxid 创建节点时的事务id
## ctime znode创建的毫秒数
## zxid  最后一次事务id
## mtime 最后一次修改的毫秒数
## pzxid 最后更新子节点的zxid
## cversion znode子节点的修改次数
## dataversion 数据变化版本
## datalength 数据长度
## numChildren 子节点数量

#新增 create 路径 内容
create /test “content”

#修改内容
set /test "content2"

#删除
delete /test

#删除包括所有子节点
deleteall /test

#单纯查看节点状态
stat /test
```

------

#### zookeeper节点类型

------

节点类型：

- Persistent 持久类型节点，在断开连接后仍保存
- Ephemeral 暂时节点，断开连接后消息 （在create时加上参数-e）

上述两类节点都可以再分为普通类型以及顺序编号节点，顺序编号就是在节点名后面加上一串序号，这串序号是递增的，随着该节点的接入次数增加。在create时加上 -s 参数就可以创建带序号的节点。（注意持久和暂时节点是共用编号顺序的）

------

#### zookeeper 监听器

------

##### 监听原理：

> 在客户端中，会有两个线程，一个负责网络通讯( connect )，另一个负责监听 ( listen )。
>
> Connect 线程将要监听的节点发送给 Zookeeper服务端, 服务端一旦监听到了某个znode的变化，就会将变化发送给客户端，客户端的listen线程将会读取该变化。
>
> 命令如下：
>
> ```
> get -w /test
> ```
>
> 注意：每次只能监听一次变化，想要再次监听变化，需要重新调用该命令

------

#### 一些实战场景

------

1. 服务器的动态上下线 :

   > 在常见的网络服务中，我们一般会有多个服务器提供服务。对于不同的客户端，会连接到不同的服务器。当服务器的状态发生变化时候，我们可以用zookeeper通知服客户端。
   >
   > 例如：假设目前客户端正在访问server1，突然server1崩溃下线了，这时server1会通知zookeeper，更改zookeeper的znode信息，一旦znode信息发生变化，客户端就会检测到，并且做出相应改动，例如将访问服务器更换到server2.

2. 分布式锁的实现

   > 在分布式情况下，数据被保存在多台服务器中，当我们要对数据进行修改时，如果访问的是不同的服务器，会出现数据不同步而造成的错误，这时候可以用zookeeper来对数据进行加锁。
   >
   > 例如：当我们想要对数据进行访问的时候，我们需要先去zookeeper中的一个znode里进行一个序列注册，然后判断自己的序列号是否是所有序列中最小的那个，如果是，则继续对数据进行访问，如果不是，则等待自己的上一个序列号的znode被释放，然后进行访问。最后，再将自己的znode删除。

------

#### 生产环境中比较适合的zookeeper服务器数量

------

10台服务器：3台zookeeper服务器

20台服务器：5台zookeeper服务器>=

如果>=100台服务器：11台zookeeper服务器



---

#### 八股

**每个znode节点中包含的内容**

Znode包含了**「存储数据(data)」**、**「访问权限(acl)」**、**「子节点引用(child)」**、**「节点状态信息(stat)」**



**三种角色**

Zookeeper 集群中**「Server有三种角色」**，Leader、Follower 和 Observer

- **「Leader」**:负责投投票的发起与决议，更新系统状态，写数据
- **「Follower」**:用于接收客户端请求并用来返回结果，在选主过程中参与投票
- **「Observer」**:可以接受客户端连接，将**「写请求转发给leader」**节点，但是不参与投票过程，只**「同步leader状态」**，主要存在目的就是**「为了提高读取效率」**



**Zookeeper 的数据一致性是依靠「ZAB协议」完成的。**

崩溃恢复: 如果leader下限 就会重新选举leader，并进行同步 知道有一半的服务器同步完成

消息广播：新的服务器加入集群的时候 需要与主leader同步



ZAB 协议包括有两种模式，分别是 **「崩溃恢复和消息广播」**。

- 崩溃恢复:当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并**「选举产生新的 Leader」** 服务器。当选举产生了新的 Leader 服务器，同时集群中已经有**「过半的机器与该 Leader 服务器完成了状态同步」**之后，ZAB 协议**「就会退出恢复模式」**。剩下未同步完成的机器会继续同步，**「直到同步完成并加入集群后该节点的服务才可用」**。
- 消息广播:当集群中**「已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步」**，那么整个服务框架就可以**「进人消息广播模式」**了。 当一台同样遵守 ZAB 协议的服务器启动后加人到集群中时，如果此时集群中**「已经存在一个 Leader 服务器在负责进行消息广播」**，那么新加人的服务器就会**「自觉地进人数据恢复模式」**：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。ZooKeeper 设计成**「只允许唯一的一个 Leader 服务器来进行事务请求」**的处理。Leader 服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的**「其他机器接收到客户端的事务」**请求，那么这些非 Leader 服务器会**「首先将这个事务请求转发给 Leader」** 服务器。





## **11.选举 leader 后是怎么进行数据同步的**

![img](https://pic4.zhimg.com/80/v2-9b4c863ff9e10cb077da9bf8b143e33f_720w.webp)

前面提到写数据是由 leader 负责的，而 leader 会将每个请求分配一个 ZXID，放入一个队列中，依次执行，每次 leader 执行完一个请求后，会记录下执行的这个 ZXID。

我们将这个队列中最大的 ZXID 称为 **「maxZXID」**，最小的 ZXID 称为 **「minZXID」**。

将 Observer 和 follower 中最新的 ZXID **「称为lastSyncZXID」**

**「proposal」**：其实就是将请求中的一些信息，ZXID 等封装到一个 proposal对象中

- **「1.差异化同步」**

- - **「触发条件」**:minZXID < lastSyncZXID < maxZXID

  - **「同步过程」**:

  - - 1).leader 向 Observer 和 follower 发送 DIFF 指令，之后就开始差异化同步
    - 2).然后把差异数据 提议 proposal 发送给 Observer 和 follower , Observer 和 follower 返回ACK表示已经完成了同步
    - 3).只要集群中过半的 Observer 和 follower 响应了 ACK 就发送一个 UPTODATE 命令
    - 4).leader 返回 ACK，同步流程结束

- **「2.回滚同步」**

- - **「触发条件」** maxZXID < lastSyncZXID

  - **「举个例子」**： a，b，c三台服务服务器 a是leader，此时队列里面最大的 ZXID 为100，a 收到请求，该 ZXID 为101，还没来得及发送同步数据 a 就挂了，b 变为leader，然后 a 恢复了，此时就需要 a 先将之前 ZXID 为101的数据回滚

  - **「同步过程」**:

  - - 1).直接回滚到 maxZXID

- **「3.回滚+差异化同步」**

- - **「触发条件」**:如果Leader刚生成一个 proposal，还没有来得及发送出去，此时Leader宕机，重新选举之后作为Follower，但是新的Leader没有这个proposal数据

  - **「举个例子」**： a，b，c三台服务服务器 a是leader，此时队列里面最大的 ZXID 为100，a 收到请求，该 ZXID 为101，还没来得及发送同步数据 a 就挂了，b 变为leader，b 又处理了3个请求，则 b 队列中最大的 ZXID 为103，然后 a 恢复了，此时就需要 a 先将之前 ZXID 为101的数据回滚，再进行同步

  - **「同步过程」**:

  - - 1).Observer 和 follower 将数据回滚
    - 2).进行差异化同步

- **「4.全量同步」**

- - **「触发条件」**

  - - 1).lastSyncZXID < minZXID
    - 2).Leader服务器上没有缓存队列，并且lastSyncZXID!=maxZXID

- - **「同步过程」**：leader 向 Observer 和 follower 发送SNAP命令，进行数据全量同步





#### 监听机制

![image-20230224145147180](/Users/whz/Library/Application Support/typora-user-images/image-20230224145147180.png)



#### 裂脑问题

心跳冗余机制 + 过半选举





##### Zookeeper为什么是强一致

对于所有客户端来说，Zookeeper中的数据都是一致的

Zookeeper之所以能够实现强一致性，是因为它采用了ZAB协议（ZooKeeper Atomic Broadcast），该协议可以保证分布式系统中所有节点的数据复制顺序一致，进而保证数据的强一致性。

在ZAB协议中，一个节点可以是Leader或Follower，Leader负责向Zookeeper集群中的其他节点广播数据，Follower负责接收Leader广播的数据，并将数据保存在本地磁盘中。当客户端向Zookeeper发送数据更新请求时，Leader会先将数据更新写入本地磁盘中的事务日志（transaction log），然后广播这条更新信息给所有的Follower节点。只有当Follower节点都将这条更新信息成功写入本地磁盘，并向Leader发送确认信息，Leader才会认为这条更新信息已经提交（committed），并将其写入内存数据库中。这样，即使在Leader节点崩溃或Follower节点崩溃的情况下，数据也不会丢失，因为它们都保存在了本地磁盘中。



##### 数据不丢失是如何保证的

1. 写操作先写入磁盘：Zookeeper将写操作的数据首先写入本地磁盘中的事务日志（transaction log），然后才会在内存中进行处理。这样即使在节点故障的情况下，数据也不会丢失，因为写操作已经被写入了磁盘。
2. ZAB协议：Zookeeper采用了ZAB协议（ZooKeeper Atomic Broadcast），该协议可以保证分布式系统中所有节点的数据复制顺序一致，进而保证数据的强一致性。在ZAB协议中，一个节点可以是Leader或Follower，Leader负责向Zookeeper集群中的其他节点广播数据，Follower负责接收Leader广播的数据，并将数据保存在本地磁盘中。只有当Follower节点都将数据成功写入本地磁盘，并向Leader发送确认信息，Leader才会认为数据已经提交，并将其写入内存数据库中。这样，即使在Leader节点崩溃或Follower节点崩溃的情况下，数据也不会丢失。
3. 数据版本控制：Zookeeper对每个数据节点都会维护一个版本号，每次数据变更都会增加版本号。在更新数据时，客户端必须指定数据的版本号，否则更新操作会失败。这样可以防止并发更新数据，从而保证数据的一致性和不丢失。



##### zoomkeeper是线程安全的吗

1. 首先，Zookeeper提供了数据版本（version）机制，用于标识数据的版本号。每次更新数据时，Zookeeper会增加数据的版本号，客户端在更新数据时必须指定数据的版本号，否则更新操作会失败。这样可以避免并发更新数据，从而保证数据的线程安全性。
2. Zookeeper还提供了一些线程安全的API，如create、delete、setData等方法，这些方法都是线程安全的，可以在多线程环境中安全地使用。



##### 集群中不同的角色

在Zookeeper集群中，每个节点都可以扮演不同的角色，包括：

1. Leader：在Zookeeper集群中，只有一个节点可以成为Leader，它负责协调集群中所有节点的工作，并处理所有的客户端读写请求。Leader通过ZAB协议将更新广播到集群中的其他节点，以保证数据的一致性。
2. Follower：在Zookeeper集群中，除了Leader节点之外的节点都可以成为Follower节点。Follower节点接收来自Leader节点的更新广播，并将更新保存在本地磁盘中，以保证数据的持久化。Follower节点可以处理客户端的读请求，但无法处理写请求，所有的写请求都需要转发给Leader节点处理。
3. Observer：Observer是一种特殊的Follower节点，它不参与Leader选举，也不参与更新广播。Observer节点可以处理客户端的读请求，但无法处理写请求。Observer节点对于集群的数据复制负载很小，适合在大规模读取场景中使用。

在Zookeeper集群中，通过Leader选举机制来选举Leader节点，当Leader节点宕机或网络异常时，集群中的其他节点会重新选举一个新的Leader节点，保证集群的高可用性和稳定性



注意 observer不参与zab协议的一致性广播 但是他会定期向另外两种节点同步数据 但是有延迟



##### 持久化机制

ZooKeeper是一个分布式协调服务，为了保证其可用性和数据的持久性，ZooKeeper提供了数据持久化功能。ZooKeeper的数据持久化可以通过两种方式实现：内存数据库和磁盘数据库。

内存数据库是ZooKeeper默认的数据存储方式。当ZooKeeper启动时，它会从磁盘加载先前保存在内存中的数据。一旦ZooKeeper开始处理客户端请求，它会将所有更改记录在内存中，并异步将这些更改写入磁盘。

磁盘数据库是ZooKeeper提供的可选数据存储方式，可以更可靠地保存ZooKeeper数据。磁盘数据库将ZooKeeper数据写入磁盘，并在启动时读取磁盘上的数据。在写入磁盘时，ZooKeeper使用了一种称为“快照”的机制，该机制会将整个ZooKeeper数据树保存到磁盘上的一个文件中。此外，ZooKeeper还使用了一种称为“事务日志”的机制，该机制记录了所有的数据更改操作，以便在需要时可以重放这些操作





##### 大量调用的处理

对于大量的客户端发起远程调用，需要频繁地访问Zookeeper，这会带来很大的开销和性能问题。为了减少这种开销，我们可以使用缓存来优化Zookeeper的访问。

具体的思路如下：

1. 数据缓存：将Zookeeper中的数据缓存到本地内存中，当需要访问Zookeeper数据时，首先从本地缓存中获取数据，减少对Zookeeper的访问。同时，为了保证数据的一致性，可以设置缓存的过期时间，当缓存过期时，需要重新从Zookeeper中获取数据。
2. 事件监听：Zookeeper提供了事件监听机制，可以在节点数据发生变化时，通知相关的客户端进行更新。可以利用这种机制，将Zookeeper的数据更新通知客户端，从而避免客户端频繁地访问Zookeeper。
3. 数据版本控制：Zookeeper提供了数据版本控制机制，可以通过对数据的版本进行比较，判断数据是否发生变化。在客户端访问Zookeeper数据时，可以先获取数据的版本号，并将其缓存到本地。当需要访问数据时，首先比较本地版本号和Zookeeper中的版本号，如果相同，则直接从本地缓存中获取数据，否则需要重新从Zookeeper中获取数据。
4. 数据分区：对于数据量较大的情况，可以将数据按照不同的分区进行存储，例如按照节点路径或节点数据进行分区。当客户端需要访问数据时，先确定数据所在的分区，然后只访问该分区中的数据，从而减少访问Zookeeper的开销。
